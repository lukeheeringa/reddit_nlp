mean_fit_time,std_fit_time,mean_score_time,std_score_time,param_vect,param_vect__max_features,param_vect__min_df,param_vect__ngram_range,param_vect__stop_words,param_vect__tokenizer,params,split0_test_score,split1_test_score,split2_test_score,split3_test_score,split4_test_score,mean_test_score,std_test_score,rank_test_score
1.3076859951019286,0.030240812911722172,0.290893030166626,0.007107072636156425,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9175108538350217,0.914616497829233,0.914616497829233,0.9319826338639653,0.918958031837916,0.9195369030390739,0.006446036888949369,1
1.2618230342864991,0.028666347181216165,0.2852789402008057,0.009023588310374456,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9117221418234442,0.91027496382055,0.9131693198263386,0.9377713458755427,0.9160636758321273,0.9178002894356005,0.010167391068162248,2
0.14300246238708497,0.007002882477051725,0.023360347747802733,0.0014047932443356519,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.914616497829233,0.9088277858176556,0.9059334298118669,0.9305354558610709,0.9247467438494935,0.916931982633864,0.009360903981937034,3
0.11251482963562012,0.007199839831217718,0.022422599792480468,0.002819259296071294,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9160636758321273,0.91027496382055,0.9030390738060782,0.9319826338639653,0.9232995658465991,0.916931982633864,0.010051377918486443,3
1.2032088756561279,0.12093683747841728,0.28947272300720217,0.02541724785798664,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9232995658465991,0.9044862518089725,0.9117221418234442,0.9305354558610709,0.9131693198263386,0.9166425470332851,0.009180174545385654,5
1.4108526229858398,0.0374126488498783,0.3088088035583496,0.026921236781489093,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9131693198263386,0.9131693198263386,0.91027496382055,0.9305354558610709,0.9160636758321273,0.9166425470332851,0.007183602689430296,5
1.2870835304260253,0.021417658862116117,0.2946046829223633,0.015844884564533274,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9160636758321273,0.9044862518089725,0.91027496382055,0.9305354558610709,0.9160636758321273,0.9154848046309695,0.008663750823210284,7
1.2302605628967285,0.017818288869867072,0.2802440643310547,0.007681540781760843,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9160636758321273,0.9044862518089725,0.91027496382055,0.9305354558610709,0.9160636758321273,0.9154848046309695,0.008663750823210284,7
1.3562305927276612,0.07340039476331169,0.3286931037902832,0.014751889850295137,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.91027496382055,0.9059334298118669,0.91027496382055,0.9305354558610709,0.918958031837916,0.9151953690303907,0.008759910825135471,9
0.14075922966003418,0.006785178487736023,0.025122928619384765,0.005243466910372068,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9175108538350217,0.9073806078147613,0.9001447178002895,0.9319826338639653,0.9175108538350217,0.914905933429812,0.010767626765695084,10
1.2798839569091798,0.04420042762275825,0.31854658126831054,0.02262635689722646,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.918958031837916,0.9030390738060782,0.91027496382055,0.9319826338639653,0.91027496382055,0.914905933429812,0.00991712978235126,10
0.07612013816833496,0.0035975920753795938,0.014658546447753907,0.0018352618201778,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9175108538350217,0.9073806078147613,0.9030390738060782,0.9290882778581766,0.9160636758321273,0.9146164978292329,0.00901442055131545,12
0.08067646026611328,0.00991655550643696,0.019953060150146484,0.006267713762508336,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9175108538350217,0.9073806078147613,0.9030390738060782,0.9290882778581766,0.9160636758321273,0.9146164978292329,0.00901442055131545,12
1.260770845413208,0.03772069002280888,0.3266029357910156,0.028317236265586392,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.918958031837916,0.894356005788712,0.9117221418234442,0.9319826338639653,0.9131693198263386,0.9140376266280754,0.01216318459591937,14
1.1764898777008057,0.062224704409302695,0.2985392093658447,0.01302472364642353,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.918958031837916,0.894356005788712,0.9117221418234442,0.9319826338639653,0.9131693198263386,0.9140376266280754,0.01216318459591937,14
0.1411074161529541,0.0034808522134502913,0.023107147216796874,0.0016191694658979365,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9232995658465991,0.9073806078147613,0.8972503617945007,0.918958031837916,0.9218523878437048,0.9137481910274964,0.009967684722270664,16
0.07303838729858399,0.00559455003612748,0.0148345947265625,0.0013151608342882308,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9131693198263386,0.8986975397973951,0.9059334298118669,0.9334298118668596,0.9175108538350217,0.9137481910274963,0.011742670202663253,17
0.07854733467102051,0.005151437601626172,0.017473936080932617,0.0013576859815684576,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9131693198263386,0.8986975397973951,0.9059334298118669,0.9334298118668596,0.9175108538350217,0.9137481910274963,0.011742670202663253,17
0.14218535423278808,0.011577233785557984,0.020869874954223634,0.0016477852674670523,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9218523878437048,0.9073806078147613,0.8972503617945007,0.918958031837916,0.9218523878437048,0.9134587554269176,0.009703649559618086,19
0.07458982467651368,0.005155591984929372,0.013848972320556641,0.0013919045263342668,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9175108538350217,0.9059334298118669,0.8972503617945007,0.9261939218523878,0.918958031837916,0.9131693198263386,0.010273944630563732,20
0.12740836143493653,0.004383747512878993,0.02460622787475586,0.0020454223955074778,TfidfVectorizer(),,2,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9160636758321273,0.9059334298118669,0.9015918958031838,0.9276410998552822,0.914616497829233,0.9131693198263386,0.009014420551315454,20
0.14866538047790528,0.0072390574800974775,0.024053287506103516,0.0037994759818221406,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9117221418234442,0.9044862518089725,0.9001447178002895,0.9261939218523878,0.9218523878437048,0.9128798842257598,0.00991712978235126,22
0.08330416679382324,0.008584838071577288,0.017105579376220703,0.0012143675952298667,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9117221418234442,0.8972503617945007,0.9088277858176556,0.9305354558610709,0.9160636758321273,0.9128798842257597,0.010806457136089425,23
1.3340322494506835,0.03009791753074336,0.3642590045928955,0.05605346875819216,TfidfVectorizer(),,2,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9030390738060782,0.914616497829233,0.9073806078147613,0.9305354558610709,0.9073806078147613,0.912590448625181,0.009712278862843694,24
0.08043913841247559,0.005923128480256432,0.018060874938964844,0.0019779607284548376,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9160636758321273,0.9030390738060782,0.9001447178002895,0.9261939218523878,0.9175108538350217,0.9125904486251809,0.009669055333424032,25
1.217739963531494,0.015878379986959578,0.30106306076049805,0.02943299126843917,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9218523878437048,0.8871201157742402,0.9117221418234442,0.9305354558610709,0.91027496382055,0.9123010130246021,0.014575604413330454,26
0.07748055458068848,0.005216038149039322,0.01566486358642578,0.000781814305734855,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.918958031837916,0.9044862518089725,0.9001447178002895,0.9261939218523878,0.9117221418234442,0.9123010130246019,0.009449972629415031,27
0.15319361686706542,0.005160257044251591,0.035010337829589844,0.004096427569122645,TfidfVectorizer(),2500,1,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9160636758321273,0.9059334298118669,0.8986975397973951,0.9261939218523878,0.9131693198263386,0.9120115774240232,0.009307053656042548,28
3.4880935668945314,1.2143984059063415,0.2659746170043945,0.011968468635383898,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9204052098408104,0.8914616497829233,0.91027496382055,0.9261939218523878,0.9117221418234442,0.9120115774240232,0.011806702202240877,28
0.14691667556762694,0.013131676462726293,0.024678134918212892,0.0024604371329618674,TfidfVectorizer(),2500,2,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9131693198263386,0.9044862518089725,0.9001447178002895,0.9232995658465991,0.9175108538350217,0.9117221418234441,0.008438425318155274,30
1.3152927875518798,0.09313115786117215,0.28756914138793943,0.019470759659016183,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.918958031837916,0.9030390738060782,0.9044862518089725,0.9232995658465991,0.9073806078147613,0.9114327062228653,0.008155717976037825,31
0.0794914722442627,0.008968935969213358,0.018170166015625,0.0017669228622081222,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.914616497829233,0.9001447178002895,0.9044862518089725,0.9218523878437048,0.914616497829233,0.9111432706222867,0.0077986648829565875,32
0.07969341278076172,0.001716043275501772,0.01700930595397949,0.001072834685587839,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.914616497829233,0.9001447178002895,0.9044862518089725,0.9232995658465991,0.9117221418234442,0.9108538350217078,0.00806274285220498,33
0.10746235847473144,0.025238041618192097,0.02178645133972168,0.006316160136892013,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9175108538350217,0.9030390738060782,0.894356005788712,0.9232995658465991,0.914616497829233,0.9105643994211288,0.010451791655726654,34
1.2118782997131348,0.04051311211301972,0.2860713958740234,0.010512511889466605,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9175108538350217,0.9001447178002895,0.9015918958031838,0.9247467438494935,0.9088277858176556,0.9105643994211288,0.00939663275296552,34
0.10728363990783692,0.011203610465136375,0.018045663833618164,0.005639986655898635,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.914616497829233,0.8900144717800289,0.9059334298118669,0.9305354558610709,0.9117221418234442,0.9105643994211288,0.013117533075108153,34
1.4433397769927978,0.0149240681675377,0.3120113849639893,0.019524505095186245,TfidfVectorizer(),,1,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9088277858176556,0.9030390738060782,0.9044862518089725,0.9305354558610709,0.9059334298118669,0.9105643994211288,0.010167391068162248,34
0.16992559432983398,0.00439932113915504,0.027829980850219725,0.004416378493405485,TfidfVectorizer(),,1,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9088277858176556,0.9015918958031838,0.9001447178002895,0.9276410998552822,0.914616497829233,0.9105643994211288,0.010001246045250937,34
0.13849120140075682,0.007637191744953135,0.025592231750488283,0.003928350098169755,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8929088277858177,0.8972503617945007,0.8986975397973951,0.9392185238784371,0.9232995658465991,0.9102749638205498,0.01795900672357577,39
0.1444225788116455,0.007126814677149859,0.02362933158874512,0.0023970704849634815,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.918958031837916,0.9073806078147613,0.8958031837916064,0.9131693198263386,0.914616497829233,0.909985528219971,0.008000159167053676,40
0.10759568214416504,0.004715976297694085,0.02156939506530762,0.00160397760463433,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.918958031837916,0.9073806078147613,0.8958031837916064,0.9131693198263386,0.914616497829233,0.909985528219971,0.008000159167053676,40
0.17897496223449708,0.04966038392693563,0.02636609077453613,0.0035383734228492717,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(1, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.918958031837916,0.9073806078147613,0.8958031837916064,0.9131693198263386,0.914616497829233,0.909985528219971,0.008000159167053676,40
1.4138708114624023,0.10613470309799142,0.34913086891174316,0.08050586494753065,TfidfVectorizer(),2500,2,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9044862518089725,0.9088277858176556,0.9044862518089725,0.9290882778581766,0.9001447178002895,0.9094066570188133,0.010216707724875729,43
0.07710108757019044,0.002387846142492879,0.016178369522094727,0.0013443791950819486,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9204052098408104,0.9001447178002895,0.8958031837916064,0.914616497829233,0.9131693198263386,0.9088277858176557,0.00928903417417146,44
0.06852555274963379,0.003225799787509499,0.014864778518676758,0.002036871837709484,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9204052098408104,0.9001447178002895,0.8958031837916064,0.914616497829233,0.9131693198263386,0.9088277858176557,0.00928903417417146,44
0.08117775917053223,0.014055312123288791,0.016615867614746094,0.002625669690700796,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(1, 1)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9204052098408104,0.9001447178002895,0.8958031837916064,0.914616497829233,0.9131693198263386,0.9088277858176557,0.00928903417417146,44
1.4272444725036622,0.04160392359888872,0.296559476852417,0.015503977858996964,TfidfVectorizer(),2500,1,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9001447178002895,0.9131693198263386,0.9044862518089725,0.9247467438494935,0.9015918958031838,0.9088277858176556,0.009152757337679831,47
1.2430038452148438,0.013986071848164298,0.3114345550537109,0.010561176818410978,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9030390738060782,0.9015918958031838,0.91027496382055,0.9131693198263386,0.9131693198263386,0.9082489146164978,0.004979638360082565,48
1.2726495265960693,0.01826221883560994,0.29099488258361816,0.013767868698048273,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9117221418234442,0.9044862518089725,0.894356005788712,0.9247467438494935,0.9059334298118669,0.9082489146164978,0.009967684722270673,48
1.2479430198669434,0.10659174256424682,0.3205425262451172,0.09133900501283349,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9131693198263386,0.8929088277858177,0.9044862518089725,0.9247467438494935,0.9044862518089725,0.907959479015919,0.01057925724022985,50
1.3088501930236816,0.04168138817234958,0.3120607852935791,0.022755811188487955,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9044862518089725,0.8929088277858177,0.9117221418234442,0.9204052098408104,0.91027496382055,0.907959479015919,0.009088462210192508,50
0.21599454879760743,0.05839133081309951,0.025304031372070313,0.0020449192267918844,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8972503617945007,0.8885672937771346,0.91027496382055,0.9276410998552822,0.9160636758321273,0.907959479015919,0.01377786423781407,50
0.1733166217803955,0.01163713414593449,0.027785634994506835,0.005803613242269807,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8972503617945007,0.8914616497829233,0.9030390738060782,0.9334298118668596,0.914616497829233,0.9079594790159188,0.01486019988769138,53
1.230031967163086,0.048966181906432635,0.3284944534301758,0.05798156212236841,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9175108538350217,0.9059334298118669,0.8986975397973951,0.9175108538350217,0.8972503617945007,0.9073806078147613,0.008779016433055361,54
0.09062886238098145,0.004063264161069486,0.016333389282226562,0.0019157793248858696,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9117221418234442,0.8986975397973951,0.9015918958031838,0.918958031837916,0.9059334298118669,0.9073806078147613,0.007264775917806718,54
1.2004408359527587,0.03502298447177007,0.2814610958099365,0.00978881821199863,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9175108538350217,0.9059334298118669,0.8986975397973951,0.9175108538350217,0.8972503617945007,0.9073806078147613,0.008779016433055361,54
0.0737600326538086,0.00528654306200417,0.015937614440917968,0.0024196748879418174,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9117221418234442,0.8986975397973951,0.9015918958031838,0.918958031837916,0.9059334298118669,0.9073806078147613,0.007264775917806718,54
0.06634726524353027,0.0015213874382492274,0.01458277702331543,0.0011839993826082756,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(1, 1)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9117221418234442,0.8986975397973951,0.9015918958031838,0.918958031837916,0.9059334298118669,0.9073806078147613,0.007264775917806718,54
1.2000180721282958,0.02770760354474652,0.27137103080749514,0.008029434346041212,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9175108538350217,0.9059334298118669,0.8986975397973951,0.9175108538350217,0.8972503617945007,0.9073806078147613,0.008779016433055361,54
1.2947606086730956,0.024995096476463828,0.30214452743530273,0.01422164869187219,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9073806078147613,0.9030390738060782,0.9030390738060782,0.9131693198263386,0.9088277858176556,0.9070911722141825,0.003817917788212122,60
1.2850756645202637,0.04516189101071194,0.30571694374084474,0.022325749552588953,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.91027496382055,0.9044862518089725,0.8958031837916064,0.9204052098408104,0.9044862518089725,0.9070911722141822,0.008104196816208395,61
1.3093238353729248,0.03702492614789419,0.2989054679870605,0.006245387642350554,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.91027496382055,0.9044862518089725,0.8958031837916064,0.9204052098408104,0.9044862518089725,0.9070911722141822,0.008104196816208395,61
1.1542912483215333,0.03022651710050128,0.2839785575866699,0.020026508310229754,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9131693198263386,0.8914616497829233,0.9044862518089725,0.9218523878437048,0.9044862518089725,0.9070911722141822,0.010126110381786357,61
1.4632343769073486,0.08532646247521593,0.3174310207366943,0.02235088667207858,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.91027496382055,0.9044862518089725,0.8958031837916064,0.9204052098408104,0.9044862518089725,0.9070911722141822,0.008104196816208395,61
0.2086045265197754,0.02149066395831589,0.037190818786621095,0.013013225652331671,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.894356005788712,0.8871201157742402,0.9030390738060782,0.9334298118668596,0.914616497829233,0.9065123010130247,0.016285736493933656,65
0.19426889419555665,0.008379651228190602,0.02791008949279785,0.0028071760201589515,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9088277858176556,0.8914616497829233,0.9030390738060782,0.9204052098408104,0.9088277858176556,0.9065123010130247,0.00940554373908649,65
0.1289726734161377,0.006598086711438327,0.025406694412231444,0.0019884896754499273,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9088277858176556,0.8914616497829233,0.9030390738060782,0.9204052098408104,0.9088277858176556,0.9065123010130247,0.00940554373908649,65
0.07900838851928711,0.007601398935260122,0.019965219497680663,0.003009010699416027,TfidfVectorizer(),2500,2,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9073806078147613,0.9001447178002895,0.894356005788712,0.9175108538350217,0.9131693198263386,0.9065123010130247,0.008418546785672358,65
0.07866444587707519,0.005566103049269726,0.01685495376586914,0.0013113921776148811,TfidfVectorizer(),,2,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9073806078147613,0.9001447178002895,0.894356005788712,0.9175108538350217,0.9131693198263386,0.9065123010130247,0.008418546785672358,65
1.3385413646698,0.0349093209866301,0.2894993782043457,0.007598331780869982,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9073806078147613,0.9044862518089725,0.894356005788712,0.918958031837916,0.9073806078147613,0.9065123010130245,0.007852191007959785,70
1.1579068183898926,0.009155653324044015,0.28178696632385253,0.0070350781840883,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.91027496382055,0.894356005788712,0.8986975397973951,0.918958031837916,0.9073806078147613,0.9059334298118669,0.008683068017366131,71
0.08284168243408203,0.003764237933739946,0.018978357315063477,0.0018303951240296378,TfidfVectorizer(),2500,1,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9117221418234442,0.8972503617945007,0.8958031837916064,0.914616497829233,0.91027496382055,0.9059334298118669,0.007820119297311903,71
1.1900759220123291,0.0179867373834448,0.29413604736328125,0.013422195018593193,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.91027496382055,0.894356005788712,0.8986975397973951,0.918958031837916,0.9073806078147613,0.9059334298118669,0.008683068017366131,71
1.141378927230835,0.017442673053266904,0.2839179992675781,0.010187722479060798,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.91027496382055,0.894356005788712,0.8986975397973951,0.918958031837916,0.9073806078147613,0.9059334298118669,0.008683068017366131,71
1.3482939720153808,0.035789317119139775,0.3133824348449707,0.024662910222442467,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9030390738060782,0.8929088277858177,0.9044862518089725,0.918958031837916,0.9088277858176556,0.9056439942112882,0.008458257132389237,75
1.3618527412414552,0.06454466920259658,0.3047849178314209,0.013403771423992496,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9117221418234442,0.8958031837916064,0.9015918958031838,0.9131693198263386,0.9059334298118669,0.9056439942112879,0.006433027719505883,76
1.2123285293579102,0.02002016146072872,0.29594974517822265,0.00914795552960036,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9117221418234442,0.8958031837916064,0.9015918958031838,0.9131693198263386,0.9059334298118669,0.9056439942112879,0.006433027719505883,76
0.09259696006774902,0.01756802643728802,0.02003960609436035,0.002539302666882917,TfidfVectorizer(),,1,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9088277858176556,0.894356005788712,0.8914616497829233,0.918958031837916,0.91027496382055,0.9047756874095514,0.010330864426960773,78
0.16774191856384277,0.0047645835292558295,0.02847766876220703,0.005074650050382246,TfidfVectorizer(),1000,1,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9044862518089725,0.9030390738060782,0.8914616497829233,0.9131693198263386,0.9117221418234442,0.9047756874095512,0.007733944553335671,79
0.18078351020812988,0.007729800000155081,0.02566952705383301,0.002370173310531894,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9030390738060782,0.8914616497829233,0.9044862518089725,0.9175108538350217,0.9059334298118669,0.9044862518089725,0.008288174276860414,80
0.21202497482299804,0.046283353725858935,0.026867151260375977,0.0018135468783422937,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9015918958031838,0.8914616497829233,0.9044862518089725,0.9175108538350217,0.9059334298118669,0.9041968162083936,0.008358627541646116,81
0.14368877410888672,0.005232581802659301,0.024731206893920898,0.004365811448140887,TfidfVectorizer(),1000,2,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9030390738060782,0.8986975397973951,0.8929088277858177,0.91027496382055,0.9131693198263386,0.9036179450072359,0.007413168436970003,82
0.18364534378051758,0.006387879314098856,0.02640843391418457,0.0017180620547774647,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(1, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9015918958031838,0.8885672937771346,0.9044862518089725,0.9160636758321273,0.9059334298118669,0.9033285094066571,0.008845561094029581,83
1.3879740238189697,0.08332682137553457,0.32694320678710936,0.006621626822225443,TfidfVectorizer(),1000,2,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8986975397973951,0.914616497829233,0.8885672937771346,0.9204052098408104,0.8929088277858177,0.9030390738060783,0.012381620628774199,84
1.4750327110290526,0.08692087067809762,0.39768123626708984,0.045836495765052845,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.894356005788712,0.8986975397973951,0.9044862518089725,0.9117221418234442,0.9059334298118669,0.9030390738060781,0.0060018643569689405,85
1.3098971366882324,0.013688689863923556,0.30143003463745116,0.01860021879978033,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8929088277858177,0.9015918958031838,0.9030390738060782,0.9117221418234442,0.9044862518089725,0.9027496382054994,0.006029715385238656,86
1.3105153560638427,0.04157581185513814,0.30727367401123046,0.020441628955114435,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.894356005788712,0.9001447178002895,0.9030390738060782,0.91027496382055,0.9059334298118669,0.9027496382054994,0.005368230677566263,86
0.0845106601715088,0.005070524966007553,0.017383813858032227,0.001832066414145134,TfidfVectorizer(),1000,1,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9015918958031838,0.894356005788712,0.8900144717800289,0.914616497829233,0.9117221418234442,0.9024602026049203,0.009538209582219457,88
0.15921397209167482,0.016698524823334622,0.02736043930053711,0.003237505587267778,TfidfVectorizer(),,2,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8885672937771346,0.8885672937771346,0.9030390738060782,0.9247467438494935,0.9059334298118669,0.9021707670043415,0.013376810429280857,89
0.08655738830566406,0.009310115078196774,0.01768040657043457,0.003519932627269701,TfidfVectorizer(),,2,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9001447178002895,0.8842257597684515,0.894356005788712,0.9218523878437048,0.91027496382055,0.9021707670043415,0.012963354950251069,89
0.07741746902465821,0.0029370330179503755,0.01939702033996582,0.002521675639648307,TfidfVectorizer(),2500,2,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9001447178002895,0.8842257597684515,0.894356005788712,0.9218523878437048,0.91027496382055,0.9021707670043415,0.012963354950251069,89
0.1479118824005127,0.013152002575976042,0.02504839897155762,0.004162923652815369,TfidfVectorizer(),1000,5,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9044862518089725,0.8958031837916064,0.8914616497829233,0.9073806078147613,0.91027496382055,0.9018813314037626,0.007113288409519257,92
0.11753816604614258,0.009748848449650587,0.023647403717041014,0.004049520548937829,TfidfVectorizer(),,5,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9044862518089725,0.8958031837916064,0.8914616497829233,0.9073806078147613,0.91027496382055,0.9018813314037626,0.007113288409519257,92
0.15067458152770996,0.008014774865214786,0.02989358901977539,0.0072628371294191744,TfidfVectorizer(),2500,5,"(1, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9044862518089725,0.8958031837916064,0.8914616497829233,0.9073806078147613,0.91027496382055,0.9018813314037626,0.007113288409519257,92
1.278353214263916,0.05042523086901327,0.2880673408508301,0.016871345316422603,TfidfVectorizer(),2500,2,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9001447178002895,0.8929088277858177,0.8914616497829233,0.9175108538350217,0.9015918958031838,0.9007235890014472,0.00927097966891286,95
1.3497138500213623,0.034494295252703366,0.31000232696533203,0.028969394126991586,TfidfVectorizer(),,2,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9001447178002895,0.8929088277858177,0.8914616497829233,0.9175108538350217,0.9015918958031838,0.9007235890014472,0.00927097966891286,95
1.2236783981323243,0.011844179308037096,0.3024695873260498,0.020835896472875905,TfidfVectorizer(),2500,1,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9044862518089725,0.8914616497829233,0.8914616497829233,0.914616497829233,0.9001447178002895,0.9004341534008684,0.008702342331909075,97
0.09553794860839844,0.01922395354438894,0.018998384475708008,0.005902409777678755,TfidfVectorizer(),1000,2,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9001447178002895,0.8958031837916064,0.8842257597684515,0.9117221418234442,0.91027496382055,0.9004341534008683,0.010084660717906995,98
0.18612079620361327,0.004197040741616051,0.029980993270874022,0.002687737847246406,TfidfVectorizer(),2500,2,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8885672937771346,0.8885672937771346,0.8958031837916064,0.9218523878437048,0.9073806078147613,0.9004341534008683,0.012728586643876143,98
0.20898914337158203,0.015069844162228752,0.033872127532958984,0.006508634017690223,TfidfVectorizer(),,1,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.894356005788712,0.8813314037626628,0.9001447178002895,0.9160636758321273,0.9088277858176556,0.9001447178002895,0.011968783286909005,100
1.4164291858673095,0.06785661652327935,0.3458245754241943,0.04302404251193055,TfidfVectorizer(),2500,5,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.91027496382055,0.8914616497829233,0.9160636758321273,0.8871201157742402,0.9001447178002895,0.011134809876510186,100
1.33409423828125,0.04332490315014847,0.3138584613800049,0.018807919286221216,TfidfVectorizer(),,2,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.894356005788712,0.8885672937771346,0.9044862518089725,0.91027496382055,0.9030390738060782,0.9001447178002895,0.007712250416482908,100
1.3226323127746582,0.028534324273687864,0.29440741539001464,0.006952910287956535,TfidfVectorizer(),1000,5,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.91027496382055,0.8914616497829233,0.9160636758321273,0.8871201157742402,0.9001447178002895,0.011134809876510186,100
1.5324368000030517,0.07616311516397414,0.3628847122192383,0.06468014806977372,TfidfVectorizer(),1000,1,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8972503617945007,0.91027496382055,0.8929088277858177,0.9088277858176556,0.8914616497829233,0.9001447178002895,0.00792652036910515,100
1.4613481521606446,0.15807768262725047,0.3237890720367432,0.024733010025498884,TfidfVectorizer(),,5,"(1, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.91027496382055,0.8914616497829233,0.9160636758321273,0.8871201157742402,0.9001447178002895,0.011134809876510186,100
0.18527660369873047,0.010842291962741623,0.029533767700195314,0.003447367298809074,TfidfVectorizer(),2500,1,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8885672937771346,0.8871201157742402,0.8972503617945007,0.9204052098408104,0.9073806078147613,0.9001447178002893,0.012449095900206413,106
0.10062708854675292,0.023133873728678325,0.018390083312988283,0.0025181757710776398,TfidfVectorizer(),1000,1,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9001447178002895,0.8929088277858177,0.8914616497829233,0.9044862518089725,0.91027496382055,0.8998552821997106,0.007054157804314864,107
0.08228163719177246,0.008942406027349257,0.019484853744506835,0.0014606606916121837,TfidfVectorizer(),1000,2,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8986975397973951,0.8958031837916064,0.8900144717800289,0.9073806078147613,0.9073806078147613,0.8998552821997106,0.006750740254524256,107
0.07835397720336915,0.0035729952126750536,0.017899370193481444,0.0013459417555275834,TfidfVectorizer(),2500,1,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.894356005788712,0.8798842257597684,0.8986975397973951,0.9175108538350217,0.9088277858176556,0.8998552821997106,0.012826928976069673,107
0.1480088710784912,0.01612512847919533,0.02687697410583496,0.003194358798950756,TfidfVectorizer(),,5,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9015918958031838,0.8842257597684515,0.8958031837916064,0.914616497829233,0.9015918958031838,0.8995658465991317,0.00984081041968163,110
0.18869571685791015,0.007005117549888745,0.03375530242919922,0.007105374645004973,TfidfVectorizer(),2500,5,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9015918958031838,0.8842257597684515,0.8958031837916064,0.914616497829233,0.9015918958031838,0.8995658465991317,0.00984081041968163,110
1.500544023513794,0.13125134587938359,0.3041811466217041,0.01356143175292079,TfidfVectorizer(),1000,1,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9030390738060782,0.8958031837916064,0.8914616497829233,0.9117221418234442,0.8958031837916064,0.8995658465991317,0.007125055644046969,110
0.0813176155090332,0.007345565526695365,0.01618170738220215,0.0008249987313062584,TfidfVectorizer(),1000,5,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9030390738060782,0.894356005788712,0.8885672937771346,0.9030390738060782,0.9059334298118669,0.898986975397974,0.006497813117407714,113
1.4030733108520508,0.04429632884118696,0.30515027046203613,0.03379371879089978,TfidfVectorizer(),,1,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9059334298118669,0.8929088277858177,0.8842257597684515,0.91027496382055,0.9015918958031838,0.898986975397974,0.009351950461015057,113
0.08296875953674317,0.009884810407870087,0.015292596817016602,0.0003550548947895951,TfidfVectorizer(),2500,5,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9030390738060782,0.894356005788712,0.8885672937771346,0.9030390738060782,0.9059334298118669,0.898986975397974,0.006497813117407714,113
0.07373485565185547,0.004231754870493997,0.015328693389892577,0.0016310173585825493,TfidfVectorizer(),,5,"(1, 1)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.9030390738060782,0.894356005788712,0.8885672937771346,0.9030390738060782,0.9059334298118669,0.898986975397974,0.006497813117407714,113
1.2658079624176026,0.021395118862258387,0.28984642028808594,0.018828785057305453,TfidfVectorizer(),1000,2,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9001447178002895,0.8929088277858177,0.8856729377713459,0.9175108538350217,0.8972503617945007,0.898697539797395,0.010595082615179805,117
0.07816843986511231,0.004083458127981465,0.017032575607299805,0.0010485074394855588,TfidfVectorizer(),,5,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9015918958031838,0.8827785817655571,0.8900144717800289,0.9088277858176556,0.9030390738060782,0.8972503617945007,0.00946768580827563,118
0.08389787673950196,0.00734409480756177,0.01645793914794922,0.0005753457476766287,TfidfVectorizer(),1000,5,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9015918958031838,0.8827785817655571,0.8900144717800289,0.9088277858176556,0.9030390738060782,0.8972503617945007,0.00946768580827563,118
0.08047966957092285,0.0032979520897103076,0.01643657684326172,0.001352674283240507,TfidfVectorizer(),2500,5,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.9015918958031838,0.8827785817655571,0.8900144717800289,0.9088277858176556,0.9030390738060782,0.8972503617945007,0.00946768580827563,118
0.08370652198791503,0.00619651367241507,0.021103858947753906,0.006687347638562467,TfidfVectorizer(),,1,"(1, 1)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8958031837916064,0.8784370477568741,0.8914616497829233,0.9160636758321273,0.9044862518089725,0.8972503617945007,0.012616205335862996,118
1.3009426593780518,0.06542202865624946,0.32585582733154295,0.013726857444777883,TfidfVectorizer(),,2,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8986975397973951,0.8813314037626628,0.8914616497829233,0.9073806078147613,0.9059334298118669,0.8969609261939219,0.009660387420132352,122
1.2610177516937255,0.04446679432126843,0.29993629455566406,0.029831645628086215,TfidfVectorizer(),2500,2,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8986975397973951,0.8813314037626628,0.8914616497829233,0.9073806078147613,0.9059334298118669,0.8969609261939219,0.009660387420132352,122
0.18598675727844238,0.00483153810858964,0.029644489288330078,0.0040898609566215926,TfidfVectorizer(),1000,5,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8958031837916064,0.8784370477568741,0.894356005788712,0.9117221418234442,0.9030390738060782,0.896671490593343,0.011006166912782346,124
0.18823580741882323,0.009272442653869006,0.03191328048706055,0.004965328334460881,TfidfVectorizer(),1000,1,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8958031837916064,0.8798842257597684,0.8914616497829233,0.91027496382055,0.9044862518089725,0.8963820549927641,0.010539589844302828,125
1.384157419204712,0.06557225710139909,0.34099903106689455,0.009552019155619368,TfidfVectorizer(),2500,1,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8929088277858177,0.8871201157742402,0.8929088277858177,0.9175108538350217,0.8900144717800289,0.8960926193921852,0.010922119979226182,126
0.19903874397277832,0.016446694000649936,0.029493951797485353,0.0035509033144645284,TfidfVectorizer(),1000,2,"(1, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8958031837916064,0.8784370477568741,0.8914616497829233,0.91027496382055,0.9044862518089725,0.8960926193921852,0.010998552821997088,126
1.3218047618865967,0.044988439223894935,0.3087038993835449,0.02547583916809132,TfidfVectorizer(),2500,2,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8856729377713459,0.8885672937771346,0.894356005788712,0.9160636758321273,0.8929088277858177,0.8955137481910276,0.01072865585693233,128
1.1843998432159424,0.03178106579750087,0.2883010387420654,0.008642205387987495,TfidfVectorizer(),2500,1,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8986975397973951,0.8755426917510853,0.8914616497829233,0.9044862518089725,0.9073806078147613,0.8955137481910274,0.011373014590094663,129
1.453420639038086,0.06350367980938125,0.347385311126709,0.018192944292980927,TfidfVectorizer(),,1,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8929088277858177,0.8871201157742402,0.8958031837916064,0.9073806078147613,0.8929088277858177,0.8952243125904487,0.006700918612324314,130
1.3502385139465332,0.037757399264378266,0.33728680610656736,0.03196430124303095,TfidfVectorizer(),,1,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.9001447178002895,0.8755426917510853,0.8871201157742402,0.9030390738060782,0.9088277858176556,0.8949348769898698,0.012024647223062818,131
1.1694826126098632,0.039270989454301096,0.30286121368408203,0.011649865116828813,TfidfVectorizer(),1000,2,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.8842257597684515,0.8856729377713459,0.9088277858176556,0.9001447178002895,0.8949348769898698,0.009180174545385637,131
1.2571375846862793,0.04794892829170681,0.2994973182678223,0.01040303015887097,TfidfVectorizer(),2500,5,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8972503617945007,0.8986975397973951,0.8813314037626628,0.9059334298118669,0.8885672937771346,0.8943560057887119,0.008537123706947961,133
1.2053618907928467,0.03677820698500651,0.27730727195739746,0.008871650025083513,TfidfVectorizer(),1000,5,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8972503617945007,0.8986975397973951,0.8813314037626628,0.9059334298118669,0.8885672937771346,0.8943560057887119,0.008537123706947961,133
1.2199977397918702,0.021745795353449834,0.29808840751647947,0.012665293688568597,TfidfVectorizer(),,5,"(1, 1)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8972503617945007,0.8986975397973951,0.8813314037626628,0.9059334298118669,0.8885672937771346,0.8943560057887119,0.008537123706947961,133
1.2724029064178466,0.057778775030263456,0.30623326301574705,0.015886530950577477,TfidfVectorizer(),1000,1,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8972503617945007,0.8827785817655571,0.8856729377713459,0.9044862518089725,0.9001447178002895,0.8940665701881331,0.008408589896575364,136
1.2763366222381591,0.03129264292419514,0.3030909538269043,0.015227832818479942,TfidfVectorizer(),2500,5,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.8827785817655571,0.8856729377713459,0.9044862518089725,0.9001447178002895,0.8937771345875543,0.008318441599536852,137
1.2129583358764648,0.06468983838701367,0.29016642570495604,0.01424731015951091,TfidfVectorizer(),1000,5,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.8827785817655571,0.8856729377713459,0.9044862518089725,0.9001447178002895,0.8937771345875543,0.008318441599536852,137
1.2132152557373046,0.02535336501459101,0.30237250328063964,0.020131443787184428,TfidfVectorizer(),,5,"(1, 1)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.8827785817655571,0.8856729377713459,0.9044862518089725,0.9001447178002895,0.8937771345875543,0.008318441599536852,137
1.3965445041656495,0.03152230766013338,0.33034844398498536,0.015235535035262291,TfidfVectorizer(),2500,5,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.8842257597684515,0.8885672937771346,0.9117221418234442,0.8856729377713459,0.8931982633863965,0.010084660717906977,140
1.345954990386963,0.06056926723334209,0.32129201889038084,0.016971064175591292,TfidfVectorizer(),,5,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8958031837916064,0.8842257597684515,0.8885672937771346,0.9117221418234442,0.8856729377713459,0.8931982633863965,0.010084660717906977,140
1.3099398136138916,0.03372741161627073,0.30436077117919924,0.008727785921540466,TfidfVectorizer(),1000,5,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8842257597684515,0.8784370477568741,0.8929088277858177,0.9059334298118669,0.8885672937771346,0.8900144717800289,0.00928903417417146,142
1.2855335235595704,0.021671765629256844,0.30323405265808107,0.012107562949126977,TfidfVectorizer(),1000,2,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8813314037626628,0.8798842257597684,0.8914616497829233,0.9044862518089725,0.8900144717800289,0.8894356005788712,0.008807597137309935,143
1.709757423400879,0.1206760083890611,0.4669209957122803,0.1226983892790373,TfidfVectorizer(),1000,1,"(1, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8827785817655571,0.8798842257597684,0.8914616497829233,0.9044862518089725,0.8885672937771346,0.889435600578871,0.00856651148303834,144
1.2345312118530274,0.018318272786971516,0.29945964813232423,0.009055814111871271,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8350217076700435,0.8408104196816208,0.8408104196816208,0.8321273516642547,0.8191027496382055,0.8335745296671492,0.00797918885793935,145
1.4223527908325195,0.04959108679500407,0.3114229679107666,0.024607748607118545,TfidfVectorizer(),,1,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8306801736613604,0.8306801736613604,0.8234442836468886,0.8350217076700435,0.8162083936324168,0.8272069464544141,0.006638115740190867,146
1.2422689914703369,0.025167871041999176,0.29138979911804197,0.020979459516783035,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8263386396526773,0.8263386396526773,0.8321273516642547,0.8118668596237337,0.8219971056439942,0.8237337192474674,0.006750740254524217,147
1.4256081104278564,0.0702877112244111,0.3533451080322266,0.031214832713188964,TfidfVectorizer(),,1,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8205499276410999,0.8306801736613604,0.8191027496382055,0.8147612156295224,0.8205499276410999,0.8211287988422576,0.005225895827863336,148
0.13823962211608887,0.007874284058482027,0.020160484313964843,0.0018864867044662546,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.808972503617945,0.8104196816208393,0.8263386396526773,0.8133140376266281,0.8176555716353111,0.8153400868306802,0.006248055903016755,149
0.15602874755859375,0.01710900182753588,0.023202180862426758,0.0032146508935263483,TfidfVectorizer(),,1,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.8060781476121563,0.8060781476121563,0.8147612156295224,0.8205499276410999,0.8060781476121563,0.8107091172214183,0.005959843786388993,150
1.2212959766387939,0.02341776334634421,0.30984973907470703,0.0220499096333955,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8133140376266281,0.808972503617945,0.8060781476121563,0.8060781476121563,0.7973950795947902,0.8063675832127352,0.005209840810419694,151
1.4816266536712646,0.10615876093169184,0.40037002563476565,0.08195046713652732,TfidfVectorizer(),,2,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8118668596237337,0.8176555716353111,0.7916063675832128,0.8176555716353111,0.7901591895803184,0.8057887120115774,0.01236130622521712,152
1.3537792205810546,0.09951876111358693,0.29946222305297854,0.016894707002575744,TfidfVectorizer(),2500,2,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8060781476121563,0.8176555716353111,0.7800289435600579,0.8162083936324168,0.7829232995658466,0.8005788712011578,0.01612548570440997,153
1.2740551471710204,0.07598720788856103,0.31148338317871094,0.013417835822951571,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.804630969609262,0.8075253256150506,0.788712011577424,0.8075253256150506,0.7800289435600579,0.7976845151953691,0.011261983478057137,154
1.300666618347168,0.046592515225037746,0.30753211975097655,0.016476143604245597,TfidfVectorizer(),2500,1,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8017366136034733,0.8060781476121563,0.784370477568741,0.8133140376266281,0.7814761215629522,0.7973950795947902,0.012415404103923156,155
1.3872445583343507,0.04527637496450441,0.31770853996276854,0.019524879255560183,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.8031837916063675,0.8017366136034733,0.7945007235890015,0.7988422575976846,0.7829232995658466,0.7962373371924747,0.007287802206665219,156
1.3747401714324952,0.019694149621742433,0.34606375694274905,0.048033941461023404,TfidfVectorizer(),2500,1,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7973950795947902,0.8002894356005789,0.7800289435600579,0.7800289435600579,0.7901591895803184,0.7895803183791607,0.00846815562276571,157
0.11167473793029785,0.006531803528673832,0.018488407135009766,0.0018511572475593517,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,1,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7829232995658466,0.7727930535455861,0.7800289435600579,0.8017366136034733,0.808972503617945,0.7892908827785818,0.013716926901386728,158
1.3725600719451905,0.04195035765369744,0.3247548580169678,0.029259713724923333,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7959479015918958,0.788712011577424,0.7916063675832128,0.7829232995658466,0.784370477568741,0.788712011577424,0.0047559122214630755,159
0.13771319389343262,0.012645917156626602,0.023376989364624023,0.0035499556100982033,TfidfVectorizer(),,1,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7771345875542692,0.7756874095513748,0.7829232995658466,0.7945007235890015,0.808972503617945,0.7878437047756874,0.012469267281353412,160
1.3477554321289062,0.03166474533492271,0.304108190536499,0.016614915591103986,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7916063675832128,0.7829232995658466,0.7872648335745297,0.7756874095513748,0.7756874095513748,0.7826338639652677,0.006301459061382719,161
1.3604716300964355,0.016697961475878778,0.2989489555358887,0.014480123069704557,TfidfVectorizer(),2500,2,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7959479015918958,0.7901591895803184,0.7756874095513748,0.7727930535455861,0.7785817655571635,0.7826338639652677,0.008892788130672758,161
1.31506667137146,0.029466001855932948,0.3115802764892578,0.01622277963089703,TfidfVectorizer(),,2,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7959479015918958,0.7901591895803184,0.7756874095513748,0.7727930535455861,0.7785817655571635,0.7826338639652677,0.008892788130672758,161
1.2372893810272216,0.01877827921949883,0.29189262390136717,0.009663746265214343,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7916063675832128,0.7829232995658466,0.7872648335745297,0.7756874095513748,0.7756874095513748,0.7826338639652677,0.006301459061382719,161
0.1627427101135254,0.02940091266037286,0.02522597312927246,0.005683387736160588,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.768451519536903,0.7670043415340086,0.8205499276410999,0.7713458755426917,0.784370477568741,0.782344428364689,0.020065215643921316,165
0.10459361076354981,0.004465302176624863,0.019006061553955077,0.0015028720033662534,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.768451519536903,0.768451519536903,0.8219971056439942,0.7713458755426917,0.7814761215629522,0.7823444283646889,0.020396483852327483,166
0.1649496078491211,0.016747293572184987,0.02514023780822754,0.005823400799777493,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.768451519536903,0.768451519536903,0.8219971056439942,0.7713458755426917,0.7800289435600579,0.78205499276411,0.02041700965389105,167
0.15297327041625977,0.012970611187077924,0.024355316162109376,0.006713019197932444,TfidfVectorizer(),2500,2,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.768451519536903,0.7742402315484804,0.8017366136034733,0.7872648335745297,0.7713458755426917,0.7806078147612157,0.01236808140041767,168
0.10945558547973633,0.0069149080325414825,0.01973719596862793,0.0023453109055837106,TfidfVectorizer(),,2,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.768451519536903,0.7742402315484804,0.8017366136034733,0.7872648335745297,0.7698986975397974,0.7803183791606367,0.012596269260196212,169
0.165878963470459,0.023412008499223723,0.029474401473999025,0.0038276665894329317,TfidfVectorizer(),2500,1,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.768451519536903,0.768451519536903,0.8002894356005789,0.7858176555716353,0.7698986975397974,0.7785817655571636,0.01268243259056823,170
1.3264226913452148,0.06531735459723792,0.31180496215820314,0.02596221772440324,TfidfVectorizer(),1000,2,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7612156295224313,0.7785817655571635,0.7568740955137482,0.7930535455861071,0.7670043415340086,0.7713458755426917,0.013072752290172171,171
1.3212507724761964,0.026752693805766472,0.3234985828399658,0.023300492136080618,TfidfVectorizer(),1000,1,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7626628075253257,0.7771345875542692,0.7554269175108539,0.7930535455861071,0.768451519536903,0.7713458755426917,0.012976273094911641,171
1.1934183597564698,0.026754089405263776,0.2825181484222412,0.007286410965127033,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7568740955137482,0.7742402315484804,0.7698986975397974,0.784370477568741,0.7713458755426917,0.7713458755426917,0.008826599526470006,171
1.2529702186584473,0.03199632717333985,0.3023348331451416,0.009498200267560285,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7525325615050651,0.7756874095513748,0.7698986975397974,0.784370477568741,0.7698986975397974,0.7704775687409552,0.010419681620839389,174
1.3816396236419677,0.03382756985453697,0.31325926780700686,0.03294840029191933,TfidfVectorizer(),1000,2,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7756874095513748,0.7742402315484804,0.7670043415340086,0.7612156295224313,0.7655571635311144,0.7687409551374819,0.005445698327706729,175
1.348865270614624,0.06456969795563196,0.28984799385070803,0.018109093214693565,TfidfVectorizer(),1000,1,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7785817655571635,0.7771345875542692,0.76410998552822,0.7568740955137482,0.7626628075253257,0.7678726483357453,0.008517475525072168,176
1.2748371601104735,0.028573796495814326,0.28452019691467284,0.008545074583130204,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.768451519536903,0.7713458755426917,0.7713458755426917,0.76410998552822,0.76410998552822,0.7678726483357453,0.003248906558703857,176
1.2929648399353026,0.031482496552087576,0.28707590103149416,0.010170210372887963,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7742402315484804,0.7670043415340086,0.7742402315484804,0.7568740955137482,0.76410998552822,0.7672937771345876,0.006561958928367362,178
1.1903053283691407,0.022424605041486793,0.2830048561096191,0.013348851455797776,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7409551374819102,0.7597684515195369,0.7568740955137482,0.7713458755426917,0.748191027496382,0.7554269175108538,0.010355162848045354,179
1.2631721496582031,0.020414808966313588,0.29214024543762207,0.010673409999097421,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7409551374819102,0.7597684515195369,0.7568740955137482,0.7713458755426917,0.748191027496382,0.7554269175108538,0.010355162848045354,179
1.3427346706390382,0.09162878542840795,0.32688088417053224,0.03073504668769307,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7409551374819102,0.7597684515195369,0.7568740955137482,0.7713458755426917,0.748191027496382,0.7554269175108538,0.010355162848045354,179
1.5102683067321778,0.0547222091421913,0.3580610275268555,0.054979144317625024,TfidfVectorizer(),2500,5,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7554269175108539,0.7496382054992764,0.7380607814761215,0.7756874095513748,0.7424023154848046,0.7522431259044863,0.013149425981592544,182
1.281066370010376,0.02880743611464313,0.29906611442565917,0.01857937709670749,TfidfVectorizer(),1000,5,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7554269175108539,0.7496382054992764,0.7380607814761215,0.7756874095513748,0.7424023154848046,0.7522431259044863,0.013149425981592544,182
1.4243719577789307,0.12474231300724321,0.29625802040100097,0.010831091233731245,TfidfVectorizer(),,5,"(2, 2)",,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7554269175108539,0.7496382054992764,0.7380607814761215,0.7756874095513748,0.7424023154848046,0.7522431259044863,0.013149425981592544,182
0.13253607749938964,0.006380561820843631,0.01989593505859375,0.0020743586605867894,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7337192474674384,0.7395079594790159,0.7901591895803184,0.7496382054992764,0.7409551374819102,0.7507959479015919,0.020330662030490255,185
0.12664871215820311,0.007239742578581099,0.023015403747558595,0.0016735159171275913,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,1,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7452966714905933,0.7250361794500724,0.7597684515195369,0.7510853835021708,0.7698986975397974,0.7502170767004341,0.015084010675305806,186
0.16348233222961425,0.030270203173868908,0.02505011558532715,0.003482653610947685,TfidfVectorizer(),1000,2,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7293777134587555,0.7452966714905933,0.7756874095513748,0.7525325615050651,0.7395079594790159,0.7484804630969609,0.015570451700506813,187
0.12418403625488281,0.00823291786017899,0.020752811431884767,0.002326032628484506,TfidfVectorizer(),2500,1,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7424023154848046,0.7264833574529667,0.7597684515195369,0.7395079594790159,0.7727930535455861,0.7481910274963821,0.0162445327935193,188
0.13809542655944823,0.0035875258823931857,0.019618129730224608,0.00250030045924009,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7264833574529667,0.7337192474674384,0.7814761215629522,0.748191027496382,0.7452966714905933,0.7470332850940664,0.018922094504985425,189
0.14574623107910156,0.011121198024994795,0.027394485473632813,0.005116795401497655,TfidfVectorizer(),1000,1,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7221418234442837,0.7322720694645442,0.7727930535455861,0.7525325615050651,0.743849493487699,0.7447178002894357,0.017404684663818126,190
0.12790913581848146,0.010410228014639575,0.02161879539489746,0.0012483132195624282,TfidfVectorizer(),1000,1,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7395079594790159,0.7250361794500724,0.7525325615050651,0.7366136034732272,0.76410998552822,0.7435600578871201,0.01349527435817627,191
0.1180598258972168,0.00853679888807322,0.021207046508789063,0.0037914161159491922,TfidfVectorizer(),1000,2,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7380607814761215,0.7250361794500724,0.7525325615050651,0.7351664254703328,0.7626628075253257,0.7426917510853835,0.013307744060865597,192
0.11437420845031739,0.0047039616400134125,0.01829833984375,0.0014609436226188138,TfidfVectorizer(),2500,2,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7380607814761215,0.7250361794500724,0.7525325615050651,0.7351664254703328,0.7626628075253257,0.7426917510853835,0.013307744060865597,192
0.0966601848602295,0.006451018990022564,0.020001029968261717,0.003617023348557467,TfidfVectorizer(),,2,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7380607814761215,0.7250361794500724,0.7525325615050651,0.7351664254703328,0.7626628075253257,0.7426917510853835,0.013307744060865597,192
0.11024460792541504,0.0027383822472115877,0.017885208129882812,0.0010088579896171564,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,1,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 1, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7366136034732272,0.7221418234442837,0.7510853835021708,0.7395079594790159,0.7612156295224313,0.7421128798842258,0.013276231480381718,195
0.11941099166870117,0.009706167400570285,0.021969032287597657,0.0031279469078506644,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,2,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7351664254703328,0.7221418234442837,0.7510853835021708,0.7380607814761215,0.7612156295224313,0.741534008683068,0.013470421161470446,196
0.11153922080993653,0.006382957174499308,0.01784482002258301,0.002054207258557144,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,2,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7351664254703328,0.7221418234442837,0.7510853835021708,0.7380607814761215,0.7612156295224313,0.741534008683068,0.013470421161470446,196
0.08911519050598145,0.005940257413975154,0.019089651107788087,0.002034577576149894,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,2,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 2, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.7351664254703328,0.7221418234442837,0.7510853835021708,0.7380607814761215,0.7612156295224313,0.741534008683068,0.013470421161470446,196
0.13891868591308593,0.0028307411024674784,0.022646999359130858,0.005269325986198891,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7178002894356006,0.7004341534008683,0.7510853835021708,0.7264833574529667,0.7206946454413893,0.723299565846599,0.016383179968839253,199
0.11702499389648438,0.013980955145081258,0.02198638916015625,0.002453256387310166,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7178002894356006,0.7004341534008683,0.7510853835021708,0.7264833574529667,0.7206946454413893,0.723299565846599,0.016383179968839253,199
0.1354668140411377,0.0098802507055982,0.019251012802124025,0.0004753687325972827,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(2, 2)",,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7178002894356006,0.7004341534008683,0.7510853835021708,0.7264833574529667,0.7206946454413893,0.723299565846599,0.016383179968839253,199
1.2531917095184326,0.032794501362855964,0.2910360813140869,0.02384970730144219,TfidfVectorizer(),1000,5,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7264833574529667,0.7221418234442837,0.7134587554269175,0.7076700434153401,0.7163531114327062,0.7172214182344427,0.006574712990796246,202
1.2905919075012207,0.060454335172868454,0.3138646125793457,0.027716711491908793,TfidfVectorizer(),,5,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7264833574529667,0.7221418234442837,0.7134587554269175,0.7076700434153401,0.7163531114327062,0.7172214182344427,0.006574712990796246,202
1.2951237201690673,0.24022963637456937,0.25347604751586916,0.08223597618740729,TfidfVectorizer(),2500,5,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7264833574529667,0.7221418234442837,0.7134587554269175,0.7076700434153401,0.7163531114327062,0.7172214182344427,0.006574712990796246,202
0.13841691017150878,0.01031398231499151,0.022012042999267577,0.005738948797839708,TfidfVectorizer(),1000,5,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7076700434153401,0.6960926193921853,0.7409551374819102,0.723589001447178,0.7134587554269175,0.7163531114327062,0.015178130943128068,205
0.09830584526062011,0.010675571578774233,0.024172019958496094,0.007924244941854713,TfidfVectorizer(),,5,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7076700434153401,0.6960926193921853,0.7409551374819102,0.723589001447178,0.7134587554269175,0.7163531114327062,0.015178130943128068,205
0.14064631462097169,0.009848507938684678,0.02345871925354004,0.002361462867312251,TfidfVectorizer(),2500,5,"(2, 2)",,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': None, 'vect__tokenizer': None}",0.7076700434153401,0.6960926193921853,0.7409551374819102,0.723589001447178,0.7134587554269175,0.7163531114327062,0.015178130943128068,205
1.3583670616149903,0.04440910597707143,0.3166968822479248,0.017718536368359846,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7206946454413893,0.7221418234442837,0.7091172214182344,0.7076700434153401,0.7134587554269175,0.714616497829233,0.005889143255969846,208
1.2268359661102295,0.02296702685406173,0.2851422309875488,0.004956722544843951,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7206946454413893,0.7221418234442837,0.7091172214182344,0.7076700434153401,0.7134587554269175,0.714616497829233,0.005889143255969846,208
1.2866555213928224,0.0617188230330721,0.31423196792602537,0.01926146242162556,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(2, 2)",english,<__main__.LemmaTokenizer object at 0x1a13778310>,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': <__main__.LemmaTokenizer object at 0x1a13778310>}",0.7206946454413893,0.7221418234442837,0.7091172214182344,0.7076700434153401,0.7134587554269175,0.714616497829233,0.005889143255969846,208
0.1231799602508545,0.014271420510858307,0.025157403945922852,0.015675062130187867,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",1000,5,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.6555716353111433,0.6555716353111433,0.6657018813314037,0.6526772793053546,0.6700434153400868,0.6599131693198264,0.00672587556504905,211
0.09231247901916503,0.009855291996122468,0.020723724365234376,0.0039458779006226454,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",,5,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.6555716353111433,0.6555716353111433,0.6657018813314037,0.6526772793053546,0.6700434153400868,0.6599131693198264,0.00672587556504905,211
0.11875190734863281,0.002456278358668112,0.0186490535736084,0.0020230684273020443,"CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>)",2500,5,"(2, 2)",english,,"{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english',
                tokenizer=<__main__.LemmaTokenizer object at 0x1a13778310>), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.6555716353111433,0.6555716353111433,0.6657018813314037,0.6526772793053546,0.6700434153400868,0.6599131693198264,0.00672587556504905,211
0.11479706764221191,0.0061090453519869435,0.019109582901000975,0.003061698149431952,TfidfVectorizer(),1000,5,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 1000, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.6555716353111433,0.6555716353111433,0.6642547033285094,0.6526772793053546,0.6685962373371924,0.6593342981186686,0.006043592769268034,214
0.1020878791809082,0.011714985521723459,0.020942544937133788,0.005812302280069746,TfidfVectorizer(),,5,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': None, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.6555716353111433,0.6555716353111433,0.6642547033285094,0.6526772793053546,0.6685962373371924,0.6593342981186686,0.006043592769268034,214
0.1382686138153076,0.014081405587662053,0.02269115447998047,0.0023812144723326725,TfidfVectorizer(),2500,5,"(2, 2)",english,,"{'vect': TfidfVectorizer(), 'vect__max_features': 2500, 'vect__min_df': 5, 'vect__ngram_range': (2, 2), 'vect__stop_words': 'english', 'vect__tokenizer': None}",0.6555716353111433,0.6555716353111433,0.6642547033285094,0.6526772793053546,0.6685962373371924,0.6593342981186686,0.006043592769268034,214
