{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Pushshift's reddit API is set as the base url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is a function that does the main pulling of posts. \n",
    "\n",
    "First it tries to read in the already saved \"posts.csv\" file, then looks specifically at the posts from the subreddit we specified as a parameter, and takes the last UTC timestap in the dataframe. If that file is not available or causes an error, a new, blank dataframe is created and the last timestamp is set to None. \n",
    "\n",
    "The API takes in several parameters when fetching a request, so those parameters are saved in a dictionary. The subreddit is the specified function argument, the size is the maximum allowed amount of 100, and if it exists, the last timestamp is added to the parameters as well. \n",
    "\n",
    "The requests module result is saved as 'res', and then the data is pulled from the json format of res. The columns taken include subreddit the post is from, the post's title, the post's text, and the UTC timestamp. These 100 rows are then added to the bottom of the already existing dataframe of posts, and the concatenation is re-saved, overwriting the previous version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posts(subreddit):\n",
    "    \"\"\"\n",
    "    Pulls 100 reddit posts and adds them to the dataset.\n",
    "    \n",
    "    Args:\n",
    "        subreddit: the name of the subreddit to pull posts from\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame: the 100 posts added to the dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv('../data/posts.csv')\n",
    "        check = data[data['subreddit'].str.lower() == subreddit] \n",
    "        before = check['created_utc'].iloc[-1]\n",
    "    except:\n",
    "        data = pd.DataFrame(columns=['subreddit', 'title', 'selftext', 'created_utc'])\n",
    "        before = None\n",
    "    \n",
    "    params = {'subreddit' : subreddit, 'size' : 100}\n",
    "    if before != None:\n",
    "        params.update({'before' : before})\n",
    "    \n",
    "    res = requests.get(url, params)\n",
    "    update = pd.DataFrame(res.json()['data'])[['subreddit', 'title', 'selftext', 'created_utc']]\n",
    "    \n",
    "    data = pd.concat([data, update], ignore_index=True)\n",
    "    data.to_csv('../data/posts.csv', index=False)\n",
    "    \n",
    "    return update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above 'update_posts' function is called for r/coffee and r/tea in conjunction to guarantee that there is an even amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_updates():\n",
    "    \"\"\"\n",
    "    Pulls and adds 100 reddit posts from each of r/coffee and r/tea\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    update_posts('coffee')\n",
    "    update_posts('tea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'run_updates' function is called in a loop until the saved dataset is at least as long as the given input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_to(n):\n",
    "    \"\"\"\n",
    "    Continually pulls and adds reddit posts to the dataset\n",
    "    \n",
    "    Args:\n",
    "        n (int): the number of posts the set must be greater than or equal to in order to stop\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    while len(pd.read_csv('../data/posts.csv')) < n:\n",
    "        run_updates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
