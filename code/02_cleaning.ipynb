{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "olympic-bible",
   "metadata": {},
   "source": [
    "#### Step 2: Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "color-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-stupid",
   "metadata": {},
   "source": [
    "To begin the cleaning process, I'll take a look at the data at a high level to see how many null values there might be and if there are any surprises about how the data types are being stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interracial-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recreational-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   subreddit    5000 non-null   object\n",
      " 1   title        5000 non-null   object\n",
      " 2   selftext     3026 non-null   object\n",
      " 3   created_utc  5000 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-pocket",
   "metadata": {},
   "source": [
    "Only about 60% of the samples taken have selftext, but otherwise the data seems to be intact. The lack of selftext won't matter anyways as I've decided to just look at the titles of posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-particular",
   "metadata": {},
   "source": [
    "The selftext will be helpful, however, in determining if a post has been removed. Because these removed posts may be off-topic posts removed by moderators, duplicates, or otherwise non-relevant, I will exclude them from the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accurate-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['selftext'] != '[removed]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-screen",
   "metadata": {},
   "source": [
    "With the removed posts removed, I can drop columns to only leave the features that we'll be using in the model: the target ('subreddit') and the title of the posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neutral-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['selftext', 'created_utc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-plate",
   "metadata": {},
   "source": [
    "Once we're down to only subreddits and titles, I want to drop any duplicate post titles that still may be in the set, as I don't want something like a recurring thread to dominate the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "frequent-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-translation",
   "metadata": {},
   "source": [
    "With our basic cleaning out of the way, we can check to see how many samples are still left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accompanied-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4607 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  4607 non-null   object\n",
      " 1   title      4607 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 108.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-disposal",
   "metadata": {},
   "source": [
    "In total it looks like we dropped 393 posts, or just under 8% of the sample. This should still leave us with enough posts to create a reasonable model, as long as our class split didn't become too skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indian-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coffee    2316\n",
       "tea       2291\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-screen",
   "metadata": {},
   "source": [
    "Looks like r/tea lost a few more posts than r/Coffee did, but the two classes are still approximately equal, and shouldn't cause any imbalance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-month",
   "metadata": {},
   "source": [
    "With cleaning complete, I'll save the new dataframe for use in EDA and modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "welsh-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/cleaned_posts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
